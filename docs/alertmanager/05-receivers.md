# ğŸ“¡ ALERTMANAGER - RECEIVERS E NOTIFICAÃ‡Ã•ES

> **MÃ³dulo 5:** Dominando canais de notificaÃ§Ã£o

---

## ğŸ“‹ CONTEÃšDO DESTE MÃ“DULO

1. [Conceitos de Receivers](#1-conceitos-de-receivers)
2. [Email](#2-email)
3. [Slack](#3-slack)
4. [PagerDuty](#4-pagerduty)
5. [Webhooks](#5-webhooks)
6. [SMS e Telegram](#6-sms-e-telegram)
7. [MÃºltiplos Canais](#7-mÃºltiplos-canais)
8. [Troubleshooting](#8-troubleshooting)

---

## 1. CONCEITOS DE RECEIVERS

### ğŸ“¡ O que sÃ£o Receivers?

> **Analogia:** Se o roteamento Ã© o **sistema postal** que decide onde entregar, os receivers sÃ£o os **mÃ©todos de entrega**: carta registrada (email), telegrama (SMS), mensageiro (Slack), ou sirene de emergÃªncia (PagerDuty).

```mermaid
flowchart LR
    A[Alerta] --> B[Roteamento]
    B --> C[Receiver]
    
    C --> D[ğŸ“§ Email]
    C --> E[ğŸ’¬ Slack]
    C --> F[ğŸ“± PagerDuty]
    C --> G[ğŸ”— Webhook]
    C --> H[ğŸ“² SMS]
    
    D --> I[ğŸ“¬ Caixa de Email]
    E --> J[ğŸ’» Canal Slack]
    F --> K[ğŸ“± App PagerDuty]
    G --> L[ğŸ–¥ï¸ Sistema Externo]
    H --> M[ğŸ“± Celular]
```

### ğŸ—ï¸ Estrutura BÃ¡sica

```yaml
receivers:
  - name: 'nome-do-receiver'        # ğŸ·ï¸ Nome Ãºnico
    email_configs:                  # ğŸ“§ ConfiguraÃ§Ãµes de email
      - to: 'admin@empresa.com'
    slack_configs:                  # ğŸ’¬ ConfiguraÃ§Ãµes do Slack
      - channel: '#alerts'
    pagerduty_configs:              # ğŸ“± ConfiguraÃ§Ãµes do PagerDuty
      - routing_key: 'abc123'
    webhook_configs:                # ğŸ”— ConfiguraÃ§Ãµes de webhook
      - url: 'http://api.empresa.com/alerts'
```

### ğŸ¯ Receiver vs Route

```yaml
# âŒ CONFUSÃƒO COMUM
route:
  receiver: 'team-email'          # â† Aponta para receiver
  
receivers:
  - name: 'team-email'            # â† Nome deve ser igual
    email_configs:
      - to: 'team@empresa.com'

# âœ… CORRETO
route:
  receiver: 'database-team'       # â† Nome da rota
  
receivers:
  - name: 'database-team'         # â† Mesmo nome
    email_configs:
      - to: 'dba@empresa.com'
```

---

## 2. EMAIL

### ğŸ“§ ConfiguraÃ§Ã£o BÃ¡sica

```yaml
# Global (compartilhado)
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@empresa.com'
  smtp_auth_username: 'alerts@empresa.com'
  smtp_auth_password: '${SMTP_PASSWORD}'
  smtp_require_tls: true

receivers:
  - name: 'email-team'
    email_configs:
      - to: 'team@empresa.com'
        subject: '[ALERTA] {{ .GroupLabels.alertname }}'
        body: |
          ğŸš¨ ALERTA DETECTADO
          
          Alerta: {{ .GroupLabels.alertname }}
          Severidade: {{ .GroupLabels.severity }}
          InstÃ¢ncia: {{ .GroupLabels.instance }}
          
          DescriÃ§Ã£o: {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
```

### ğŸ“¨ ConfiguraÃ§Ãµes AvanÃ§adas

```yaml
receivers:
  - name: 'advanced-email'
    email_configs:
      # ========================================
      # ğŸ‘¥ DESTINATÃRIOS
      # ========================================
      - to: 'admin@empresa.com,team@empresa.com'  # MÃºltiplos emails
        cc: 'manager@empresa.com'                 # CÃ³pia
        bcc: 'audit@empresa.com'                  # CÃ³pia oculta
        
        # ========================================
        # ğŸ“ CONTEÃšDO
        # ========================================
        subject: |
          [{{ .Status | toUpper }}] {{ .GroupLabels.alertname }} 
          ({{ .Alerts | len }} alertas)
        
        body: |
          <!DOCTYPE html>
          <html>
          <head>
              <style>
                  .critical { color: #ff0000; font-weight: bold; }
                  .warning { color: #ff8800; }
                  .info { color: #0088ff; }
              </style>
          </head>
          <body>
              <h2>ğŸš¨ RelatÃ³rio de Alertas</h2>
              
              {{ if gt (len .Alerts.Firing) 0 }}
              <h3 class="critical">ğŸ”¥ ALERTAS ATIVOS ({{ len .Alerts.Firing }})</h3>
              <ul>
              {{ range .Alerts.Firing }}
                  <li>
                      <strong>{{ .Labels.alertname }}</strong><br>
                      InstÃ¢ncia: {{ .Labels.instance }}<br>
                      Severidade: <span class="{{ .Labels.severity }}">{{ .Labels.severity }}</span><br>
                      DescriÃ§Ã£o: {{ .Annotations.description }}<br>
                      Iniciado: {{ .StartsAt.Format "2006-01-02 15:04:05" }}<br>
                  </li>
              {{ end }}
              </ul>
              {{ end }}
              
              {{ if gt (len .Alerts.Resolved) 0 }}
              <h3 style="color: green;">âœ… ALERTAS RESOLVIDOS ({{ len .Alerts.Resolved }})</h3>
              <ul>
              {{ range .Alerts.Resolved }}
                  <li>
                      <strong>{{ .Labels.alertname }}</strong><br>
                      Resolvido: {{ .EndsAt.Format "2006-01-02 15:04:05" }}<br>
                  </li>
              {{ end }}
              </ul>
              {{ end }}
              
              <hr>
              <p><small>Enviado pelo Alertmanager em {{ now.Format "2006-01-02 15:04:05" }}</small></p>
          </body>
          </html>
        
        # ========================================
        # ğŸ”§ CONFIGURAÃ‡Ã•ES TÃ‰CNICAS
        # ========================================
        headers:
          X-Priority: '1'                         # Alta prioridade
          X-Mailer: 'Alertmanager'
          Reply-To: 'noreply@empresa.com'
        
        # ========================================
        # ğŸ”’ SMTP ESPECÃFICO (sobrescreve global)
        # ========================================
        smarthost: 'smtp.empresa.com:25'         # SMTP especÃ­fico
        from: 'alertas@empresa.com'              # From especÃ­fico
        auth_username: 'alertas@empresa.com'
        auth_password: '${EMPRESA_SMTP_PASSWORD}'
        require_tls: false                       # Servidor interno
```

### ğŸ“§ Templates de Email

```yaml
# templates/email.tmpl
{{ define "email.subject" }}
[{{ .Status | toUpper }}] {{ .GroupLabels.alertname }} - {{ .GroupLabels.instance }}
{{ end }}

{{ define "email.body" }}
ğŸš¨ RELATÃ“RIO DE ALERTAS
========================

Resumo:
- Total de alertas: {{ .Alerts | len }}
- Alertas ativos: {{ .Alerts.Firing | len }}
- Alertas resolvidos: {{ .Alerts.Resolved | len }}
- HorÃ¡rio: {{ now.Format "2006-01-02 15:04:05" }}

{{ if gt (len .Alerts.Firing) 0 }}
ğŸ”¥ ALERTAS ATIVOS:
{{ range .Alerts.Firing }}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Alerta: {{ .Labels.alertname }}
Severidade: {{ .Labels.severity | toUpper }}
InstÃ¢ncia: {{ .Labels.instance }}
DescriÃ§Ã£o: {{ .Annotations.description }}
Iniciado: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
DuraÃ§Ã£o: {{ .StartsAt | since }}
Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
{{ end }}
{{ end }}

{{ if gt (len .Alerts.Resolved) 0 }}
âœ… ALERTAS RESOLVIDOS:
{{ range .Alerts.Resolved }}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Alerta: {{ .Labels.alertname }}
Resolvido: {{ .EndsAt.Format "2006-01-02 15:04:05" }}
DuraÃ§Ã£o total: {{ .StartsAt | since }}
{{ end }}
{{ end }}

ğŸ”— Links Ãºteis:
- Alertmanager: http://alertmanager.empresa.com:9093
- Prometheus: http://prometheus.empresa.com:9090
- Grafana: http://grafana.empresa.com:3000

--
Enviado automaticamente pelo Alertmanager
{{ end }}

# Usar no receiver
receivers:
  - name: 'templated-email'
    email_configs:
      - to: 'team@empresa.com'
        subject: '{{ template "email.subject" . }}'
        body: '{{ template "email.body" . }}'
```

---

## 3. SLACK

### ğŸ’¬ ConfiguraÃ§Ã£o BÃ¡sica

```yaml
# Global
global:
  slack_api_url: 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX'

receivers:
  - name: 'slack-team'
    slack_configs:
      - channel: '#alerts'                    # Canal pÃºblico
        username: 'AlertBot'                  # Nome do bot
        icon_emoji: ':warning:'               # Emoji do bot
        title: 'ğŸš¨ {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alerta:* {{ .Labels.alertname }}
          *Severidade:* {{ .Labels.severity }}
          *InstÃ¢ncia:* {{ .Labels.instance }}
          *DescriÃ§Ã£o:* {{ .Annotations.description }}
          {{ end }}
```

### ğŸ¨ Slack AvanÃ§ado com Cores

```yaml
receivers:
  - name: 'slack-advanced'
    slack_configs:
      - channel: '#alerts-critical'
        username: 'AlertManager'
        icon_url: 'https://empresa.com/alertmanager-icon.png'
        
        # ========================================
        # ğŸ¨ CORES POR SEVERIDADE
        # ========================================
        color: |
          {{ if eq .Status "firing" }}
            {{ if eq .GroupLabels.severity "critical" }}danger{{ end }}
            {{ if eq .GroupLabels.severity "warning" }}warning{{ end }}
            {{ if eq .GroupLabels.severity "info" }}good{{ end }}
          {{ else }}good{{ end }}
        
        # ========================================
        # ğŸ“ TÃTULO DINÃ‚MICO
        # ========================================
        title: |
          {{ if eq .Status "firing" }}
            ğŸš¨ [{{ .GroupLabels.severity | toUpper }}] {{ .GroupLabels.alertname }}
          {{ else }}
            âœ… [RESOLVIDO] {{ .GroupLabels.alertname }}
          {{ end }}
        
        title_link: 'http://alertmanager.empresa.com:9093'
        
        # ========================================
        # ğŸ“Š TEXTO DETALHADO
        # ========================================
        text: |
          {{ if gt (len .Alerts.Firing) 0 }}
          *ğŸ”¥ Alertas Ativos: {{ len .Alerts.Firing }}*
          {{ range .Alerts.Firing }}
          
          â€¢ *{{ .Labels.alertname }}*
            ğŸ“ InstÃ¢ncia: `{{ .Labels.instance }}`
            âš ï¸ Severidade: `{{ .Labels.severity }}`
            ğŸ“ DescriÃ§Ã£o: {{ .Annotations.description }}
            â° Iniciado: {{ .StartsAt.Format "15:04:05" }}
            ğŸ”— <http://prometheus.empresa.com:9090/graph?g0.expr={{ .GeneratorURL | reReplaceAll ".*expr=([^&]*).*" "$1" | urlquery }}|Ver no Prometheus>
          {{ end }}
          {{ end }}
          
          {{ if gt (len .Alerts.Resolved) 0 }}
          *âœ… Alertas Resolvidos: {{ len .Alerts.Resolved }}*
          {{ range .Alerts.Resolved }}
          â€¢ {{ .Labels.alertname }} (resolvido Ã s {{ .EndsAt.Format "15:04:05" }})
          {{ end }}
          {{ end }}
        
        # ========================================
        # ğŸ“ CAMPOS ESTRUTURADOS
        # ========================================
        fields:
          - title: 'Ambiente'
            value: '{{ .GroupLabels.environment | default "N/A" }}'
            short: true
          - title: 'Equipe'
            value: '{{ .GroupLabels.team | default "N/A" }}'
            short: true
          - title: 'ServiÃ§o'
            value: '{{ .GroupLabels.service | default "N/A" }}'
            short: true
          - title: 'Total de Alertas'
            value: '{{ .Alerts | len }}'
            short: true
        
        # ========================================
        # ğŸ”§ CONFIGURAÃ‡Ã•ES TÃ‰CNICAS
        # ========================================
        send_resolved: true                   # Enviar quando resolver
        http_config:
          proxy_url: 'http://proxy.empresa.com:8080'
```

### ğŸ¯ Slack por Severidade

```yaml
receivers:
  # ğŸ”´ CrÃ­tico - Canal especÃ­fico com @here
  - name: 'slack-critical'
    slack_configs:
      - channel: '#alerts-critical'
        color: 'danger'
        title: 'ğŸš¨ CRÃTICO: {{ .GroupLabels.alertname }}'
        text: |
          <!here> Alerta crÃ­tico detectado!
          
          {{ range .Alerts.Firing }}
          *InstÃ¢ncia:* {{ .Labels.instance }}
          *DescriÃ§Ã£o:* {{ .Annotations.description }}
          *AÃ§Ã£o necessÃ¡ria:* {{ .Annotations.runbook_url | default "Verificar imediatamente" }}
          {{ end }}
  
  # ğŸŸ¡ Warning - Canal geral
  - name: 'slack-warning'
    slack_configs:
      - channel: '#alerts'
        color: 'warning'
        title: 'âš ï¸ Warning: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts.Firing }}
          InstÃ¢ncia: {{ .Labels.instance }}
          DescriÃ§Ã£o: {{ .Annotations.description }}
          {{ end }}
  
  # ğŸ”µ Info - Canal de logs
  - name: 'slack-info'
    slack_configs:
      - channel: '#logs'
        color: 'good'
        title: 'â„¹ï¸ Info: {{ .GroupLabels.alertname }}'
        text: 'Alerta informativo - {{ .GroupLabels.alertname }}'
```

---

## 4. PAGERDUTY

### ğŸ“± ConfiguraÃ§Ã£o BÃ¡sica

```yaml
# Global
global:
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

receivers:
  - name: 'pagerduty-oncall'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'   # Integration Key
        description: '{{ .GroupLabels.alertname }} - {{ .GroupLabels.instance }}'
        severity: |
          {{ if eq .GroupLabels.severity "critical" }}critical{{ end }}
          {{ if eq .GroupLabels.severity "warning" }}warning{{ end }}
          {{ if eq .GroupLabels.severity "info" }}info{{ end }}
```

### ğŸš¨ PagerDuty AvanÃ§ado

```yaml
receivers:
  - name: 'pagerduty-advanced'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        
        # ========================================
        # ğŸ“ DESCRIÃ‡ÃƒO DETALHADA
        # ========================================
        description: |
          [{{ .GroupLabels.severity | toUpper }}] {{ .GroupLabels.alertname }}
          InstÃ¢ncia: {{ .GroupLabels.instance }}
          Ambiente: {{ .GroupLabels.environment }}
        
        # ========================================
        # ğŸ·ï¸ SEVERIDADE MAPEADA
        # ========================================
        severity: |
          {{ if eq .GroupLabels.severity "critical" }}critical
          {{ else if eq .GroupLabels.severity "warning" }}warning
          {{ else if eq .GroupLabels.severity "info" }}info
          {{ else }}error{{ end }}
        
        # ========================================
        # ğŸ”— LINKS ÃšTEIS
        # ========================================
        client: 'Alertmanager'
        client_url: 'http://alertmanager.empresa.com:9093'
        
        # ========================================
        # ğŸ“Š DETALHES CUSTOMIZADOS
        # ========================================
        details:
          alertname: '{{ .GroupLabels.alertname }}'
          instance: '{{ .GroupLabels.instance }}'
          severity: '{{ .GroupLabels.severity }}'
          environment: '{{ .GroupLabels.environment }}'
          team: '{{ .GroupLabels.team }}'
          firing_alerts: '{{ len .Alerts.Firing }}'
          resolved_alerts: '{{ len .Alerts.Resolved }}'
          started_at: '{{ range .Alerts.Firing }}{{ .StartsAt.Format "2006-01-02 15:04:05" }}{{ end }}'
          description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
          runbook: '{{ range .Alerts }}{{ .Annotations.runbook_url }}{{ end }}'
        
        # ========================================
        # ğŸ¯ IMAGENS E CONTEXTO
        # ========================================
        images:
          - src: 'http://grafana.empresa.com/render/d/alerts/alert-dashboard?panelId=1&width=400&height=200'
            alt: 'GrÃ¡fico do alerta'
            href: 'http://grafana.empresa.com/d/alerts/alert-dashboard'
        
        links:
          - href: 'http://prometheus.empresa.com:9090/graph'
            text: 'Ver no Prometheus'
          - href: 'http://grafana.empresa.com/d/alerts/alert-dashboard'
            text: 'Dashboard de Alertas'
          - href: '{{ range .Alerts }}{{ .Annotations.runbook_url }}{{ end }}'
            text: 'Runbook'
```

### ğŸ”„ PagerDuty com Escalation

```yaml
receivers:
  # NÃ­vel 1 - Equipe primÃ¡ria
  - name: 'pagerduty-level1'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_L1_KEY}'
        description: '[L1] {{ .GroupLabels.alertname }}'
        severity: 'warning'
  
  # NÃ­vel 2 - Supervisores (apÃ³s 15min)
  - name: 'pagerduty-level2'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_L2_KEY}'
        description: '[L2 ESCALATION] {{ .GroupLabels.alertname }}'
        severity: 'error'
  
  # NÃ­vel 3 - GerÃªncia (apÃ³s 30min)
  - name: 'pagerduty-level3'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_L3_KEY}'
        description: '[L3 CRITICAL ESCALATION] {{ .GroupLabels.alertname }}'
        severity: 'critical'
```

---

## 5. WEBHOOKS

### ğŸ”— Webhook BÃ¡sico

```yaml
receivers:
  - name: 'webhook-basic'
    webhook_configs:
      - url: 'http://api.empresa.com/alerts'
        send_resolved: true
        http_config:
          basic_auth:
            username: 'alertmanager'
            password: '${WEBHOOK_PASSWORD}'
```

### ğŸ¯ Webhook AvanÃ§ado

```yaml
receivers:
  - name: 'webhook-advanced'
    webhook_configs:
      - url: 'http://api.empresa.com/v2/alerts'
        
        # ========================================
        # ğŸ”’ AUTENTICAÃ‡ÃƒO
        # ========================================
        http_config:
          authorization:
            type: 'Bearer'
            credentials: '${API_TOKEN}'
          
          # Headers customizados
          headers:
            Content-Type: 'application/json'
            X-Source: 'Alertmanager'
            X-Environment: 'production'
        
        # ========================================
        # â° CONFIGURAÃ‡Ã•ES DE TIMEOUT
        # ========================================
        max_alerts: 10                        # MÃ¡ximo de alertas por request
        send_resolved: true                   # Enviar resoluÃ§Ãµes
        
        # ========================================
        # ğŸ”„ RETRY E TIMEOUT
        # ========================================
        http_config:
          timeout: 30s
          proxy_url: 'http://proxy.empresa.com:8080'
          tls_config:
            insecure_skip_verify: false
            ca_file: '/etc/ssl/certs/ca.pem'
            cert_file: '/etc/ssl/certs/client.pem'
            key_file: '/etc/ssl/private/client.key'
```

### ğŸ“Š Webhook para Sistemas EspecÃ­ficos

```yaml
receivers:
  # ğŸ“ˆ Grafana Annotations
  - name: 'grafana-annotations'
    webhook_configs:
      - url: 'http://grafana:3000/api/annotations'
        http_config:
          authorization:
            type: 'Bearer'
            credentials: '${GRAFANA_API_KEY}'
        # Payload serÃ¡ o JSON padrÃ£o do Alertmanager
  
  # ğŸ“Š InfluxDB
  - name: 'influxdb-metrics'
    webhook_configs:
      - url: 'http://influxdb:8086/write?db=alerts'
        http_config:
          basic_auth:
            username: 'alertmanager'
            password: '${INFLUXDB_PASSWORD}'
  
  # ğŸ« Jira (criar tickets)
  - name: 'jira-tickets'
    webhook_configs:
      - url: 'http://jira-webhook-service:8080/create-ticket'
        http_config:
          basic_auth:
            username: 'jira-bot'
            password: '${JIRA_PASSWORD}'
  
  # ğŸ’¬ Microsoft Teams
  - name: 'teams-webhook'
    webhook_configs:
      - url: '${TEAMS_WEBHOOK_URL}'
        send_resolved: true
```

### ğŸ”§ Webhook Personalizado com Template

```yaml
# Criar um serviÃ§o intermediÃ¡rio que recebe o JSON do Alertmanager
# e transforma para o formato desejado

# docker-compose.yml
services:
  webhook-transformer:
    image: nginx:alpine
    volumes:
      - ./webhook-transformer.lua:/etc/nginx/webhook.lua
    ports:
      - "8080:80"

# webhook-transformer.lua (usando OpenResty)
local json = require "cjson"

-- Receber JSON do Alertmanager
local body = ngx.req.get_body_data()
local alert_data = json.decode(body)

-- Transformar para formato customizado
local custom_payload = {
    timestamp = os.time(),
    source = "alertmanager",
    alerts = {}
}

for _, alert in ipairs(alert_data.alerts) do
    table.insert(custom_payload.alerts, {
        name = alert.labels.alertname,
        severity = alert.labels.severity,
        instance = alert.labels.instance,
        status = alert_data.status,
        description = alert.annotations.description
    })
end

-- Enviar para sistema final
local http = require "resty.http"
local httpc = http.new()
local res, err = httpc:request_uri("http://final-system/api/alerts", {
    method = "POST",
    body = json.encode(custom_payload),
    headers = {
        ["Content-Type"] = "application/json",
        ["Authorization"] = "Bearer " .. os.getenv("API_TOKEN")
    }
})

ngx.status = 200
ngx.say("OK")
```

---

## 6. SMS E TELEGRAM

### ğŸ“² SMS via Webhook

```yaml
# Usando serviÃ§o SMS (Twilio, AWS SNS, etc.)
receivers:
  - name: 'sms-critical'
    webhook_configs:
      - url: 'http://sms-service:8080/send'
        http_config:
          basic_auth:
            username: 'alertmanager'
            password: '${SMS_SERVICE_PASSWORD}'
        # O serviÃ§o SMS receberÃ¡ o JSON e enviarÃ¡ SMS
```

### ğŸ¤– Telegram Bot

```yaml
# Usando webhook para Telegram Bot
receivers:
  - name: 'telegram-alerts'
    webhook_configs:
      - url: 'http://telegram-bot:8080/alert'
        send_resolved: true

# telegram-bot service (Python exemplo)
# app.py
from flask import Flask, request
import requests
import json

app = Flask(__name__)

TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')

@app.route('/alert', methods=['POST'])
def handle_alert():
    data = request.json
    
    # Formatar mensagem
    if data['status'] == 'firing':
        emoji = 'ğŸš¨'
        status = 'ATIVO'
    else:
        emoji = 'âœ…'
        status = 'RESOLVIDO'
    
    message = f"""{emoji} ALERTA {status}
    
Alerta: {data['groupLabels']['alertname']}
Severidade: {data['groupLabels']['severity']}
InstÃ¢ncia: {data['groupLabels']['instance']}

Total de alertas: {len(data['alerts'])}
"""
    
    # Enviar para Telegram
    telegram_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {
        'chat_id': TELEGRAM_CHAT_ID,
        'text': message,
        'parse_mode': 'HTML'
    }
    
    requests.post(telegram_url, json=payload)
    return 'OK'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

---

## 7. MÃšLTIPLOS CANAIS

### ğŸ¯ Receiver com MÃºltiplos Canais

```yaml
receivers:
  - name: 'multi-channel-critical'
    # ğŸ“§ Email para documentaÃ§Ã£o
    email_configs:
      - to: 'alerts-log@empresa.com'
        subject: '[CRÃTICO] {{ .GroupLabels.alertname }}'
        body: 'Alerta crÃ­tico registrado para auditoria.'
    
    # ğŸ’¬ Slack para equipe
    slack_configs:
      - channel: '#alerts-critical'
        color: 'danger'
        title: 'ğŸš¨ CRÃTICO: {{ .GroupLabels.alertname }}'
        text: '<!here> Alerta crÃ­tico detectado!'
    
    # ğŸ“± PagerDuty para on-call
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_CRITICAL_KEY}'
        description: 'CRÃTICO: {{ .GroupLabels.alertname }}'
        severity: 'critical'
    
    # ğŸ”— Webhook para sistema de tickets
    webhook_configs:
      - url: 'http://ticket-system:8080/create'
        http_config:
          basic_auth:
            username: 'alertmanager'
            password: '${TICKET_SYSTEM_PASSWORD}'
```

### ğŸ­ Canais por Contexto

```yaml
receivers:
  # ğŸŒ… HorÃ¡rio comercial - Todos os canais
  - name: 'business-hours'
    email_configs:
      - to: 'team@empresa.com'
    slack_configs:
      - channel: '#alerts'
    webhook_configs:
      - url: 'http://dashboard:8080/alerts'
  
  # ğŸŒ™ Fora do horÃ¡rio - SÃ³ crÃ­ticos
  - name: 'after-hours'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_ONCALL_KEY}'
        description: 'Alerta fora do horÃ¡rio: {{ .GroupLabels.alertname }}'
    email_configs:
      - to: 'oncall@empresa.com'
        subject: '[FORA DO HORÃRIO] {{ .GroupLabels.alertname }}'
  
  # ğŸ¯ ProduÃ§Ã£o - MÃ¡xima redundÃ¢ncia
  - name: 'production-critical'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_PROD_KEY}'
    slack_configs:
      - channel: '#prod-alerts'
    email_configs:
      - to: 'prod-team@empresa.com'
    webhook_configs:
      - url: 'http://incident-manager:8080/create'
    # SMS via webhook
    webhook_configs:
      - url: 'http://sms-service:8080/emergency'
```

### ğŸ”„ Fallback e RedundÃ¢ncia

```yaml
receivers:
  - name: 'redundant-alerts'
    # Canal primÃ¡rio
    slack_configs:
      - channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        # Se Slack falhar, nÃ£o afeta outros canais
    
    # Canal secundÃ¡rio (sempre funciona)
    email_configs:
      - to: 'backup-alerts@empresa.com'
        subject: '[BACKUP] {{ .GroupLabels.alertname }}'
    
    # Canal terciÃ¡rio (webhook confiÃ¡vel)
    webhook_configs:
      - url: 'http://reliable-service:8080/alerts'
        http_config:
          timeout: 10s
        max_alerts: 1  # Um por vez para garantir entrega
```

---

## 8. TROUBLESHOOTING

### ğŸ” Problemas Comuns

#### ğŸ“§ Email nÃ£o chega

```bash
# Verificar logs do Alertmanager
docker-compose logs alertmanager | grep -i smtp

# Testar SMTP manualmente
telnet smtp.gmail.com 587

# Verificar configuraÃ§Ã£o
amtool config check alertmanager.yml

# Testar receiver especÃ­fico
amtool alert add alertname="TestEmail" severity="warning"
```

#### ğŸ’¬ Slack nÃ£o funciona

```bash
# Verificar webhook URL
curl -X POST -H 'Content-type: application/json' \
  --data '{"text":"Teste do Alertmanager"}' \
  https://hooks.slack.com/services/T00/B00/XXX

# Verificar logs
docker-compose logs alertmanager | grep -i slack

# Verificar permissÃµes do bot
# - Bot deve estar no canal
# - Webhook deve ter permissÃµes corretas
```

#### ğŸ“± PagerDuty nÃ£o dispara

```bash
# Testar integration key
curl -X POST https://events.pagerduty.com/v2/enqueue \
  -H 'Content-Type: application/json' \
  -d '{
    "routing_key": "YOUR_ROUTING_KEY",
    "event_action": "trigger",
    "payload": {
      "summary": "Teste do Alertmanager",
      "severity": "critical",
      "source": "alertmanager"
    }
  }'

# Verificar se integration estÃ¡ ativa no PagerDuty
# Verificar se routing_key estÃ¡ correto
```

### ğŸ§ª Testando Receivers

```bash
# Enviar alerta de teste
amtool alert add \
  alertname="TestReceiver" \
  severity="warning" \
  instance="test-instance" \
  --alertmanager.url=http://localhost:9093

# Verificar se alerta foi recebido
amtool alert query --alertmanager.url=http://localhost:9093

# Verificar roteamento
amtool config routes test \
  --config.file=alertmanager.yml \
  alertname="TestReceiver" severity="warning"

# Silenciar alerta de teste
amtool silence add \
  alertname="TestReceiver" \
  --duration=1h \
  --comment="Teste finalizado"
```

### ğŸ“Š Monitoramento dos Receivers

```yaml
# Adicionar mÃ©tricas no Prometheus
# prometheus.yml
scrape_configs:
  - job_name: 'alertmanager'
    static_configs:
      - targets: ['alertmanager:9093']

# Queries Ãºteis
# NotificaÃ§Ãµes enviadas por receiver
rate(alertmanager_notifications_total[5m])

# Falhas de notificaÃ§Ã£o
rate(alertmanager_notifications_failed_total[5m])

# LatÃªncia de notificaÃ§Ãµes
histogram_quantile(0.95, rate(alertmanager_notification_latency_seconds_bucket[5m]))
```

---

## ğŸ¯ RESUMO DO MÃ“DULO

### âœ… O que vocÃª aprendeu:

1. **Conceitos de receivers** - Como funcionam os canais de notificaÃ§Ã£o
2. **Email** - ConfiguraÃ§Ã£o SMTP e templates HTML
3. **Slack** - IntegraÃ§Ã£o com cores, campos e formataÃ§Ã£o
4. **PagerDuty** - Alertas crÃ­ticos com escalation
5. **Webhooks** - IntegraÃ§Ã£o com sistemas externos
6. **SMS/Telegram** - Canais alternativos via webhook
7. **MÃºltiplos canais** - RedundÃ¢ncia e fallback
8. **Troubleshooting** - DiagnÃ³stico e resoluÃ§Ã£o de problemas

### ğŸ”§ Principais tipos de receiver:
- **email_configs** - NotificaÃ§Ãµes por email
- **slack_configs** - IntegraÃ§Ã£o com Slack
- **pagerduty_configs** - Alertas crÃ­ticos
- **webhook_configs** - Sistemas personalizados

### ğŸš€ PrÃ³ximos Passos

Agora que vocÃª domina os receivers, vamos aprender sobre **silenciamento e inibiÃ§Ã£o**:

**PrÃ³ximo mÃ³dulo:** [06-silenciamento.md](06-silenciamento.md) - Silenciamento e inibiÃ§Ã£o de alertas

---

## ğŸ”— Links Relacionados

- **[Anterior: Roteamento](04-roteamento.md)**
- **[PrÃ³ximo: Silenciamento](06-silenciamento.md)**
- **[Voltar ao Ãndice](README.md)**